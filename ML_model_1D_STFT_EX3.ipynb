{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/dataset/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd() + \"/dataset/\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/dataset//train\n",
      "/root/dataset//val\n",
      "/root/dataset//test\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = path+\"/train\"\n",
    "VALIDATION_DIR = path + \"/val\"\n",
    "TEST_DIR = path + \"/test\"\n",
    "\n",
    "print(TRAINING_DIR)\n",
    "print(VALIDATION_DIR)\n",
    "print(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(path):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for folder in os.listdir(path):\n",
    "        for file in os.listdir(os.path.join(path, folder)):\n",
    "            feature = np.load(os.path.join(path, folder, file))\n",
    "            label = folder\n",
    "            X.append(feature)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = data_loader(TRAINING_DIR)\n",
    "X_val, Y_val = data_loader(VALIDATION_DIR)\n",
    "X_test, Y_test = data_loader(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14000 training samples and 14000 training labels\n",
      "There are 2000 validation samples and 2000 validation labels\n",
      "There are 4000 testing samples and 4000 testing labels\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} training samples and {} training labels\".format(len(X_train), len(Y_train)))\n",
    "print(\"There are {} validation samples and {} validation labels\".format(len(X_val), len(Y_val)))\n",
    "print(\"There are {} testing samples and {} testing labels\".format(len(X_test), len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (14000, 64575)\n",
      "Shape of X_val: (2000, 64575)\n",
      "Shape of X_test: (4000, 64575)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_val = np.array(Y_val)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "print(\"Shape of X_train: {}\".format(X_train.shape))\n",
    "print(\"Shape of X_val: {}\".format(X_val.shape))\n",
    "print(\"Shape of X_test: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset\n",
    "shuffle_index = np.random.permutation(len(X_train))\n",
    "X_train, Y_train = X_train[shuffle_index], Y_train[shuffle_index]\n",
    "shuffle_index = np.random.permutation(len(X_val))\n",
    "X_val, Y_val = X_val[shuffle_index], Y_val[shuffle_index]\n",
    "shuffle_index = np.random.permutation(len(X_test))\n",
    "X_test, Y_test = X_test[shuffle_index], Y_test[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "Y_train = label_encoder.fit_transform(Y_train)\n",
    "Y_val = label_encoder.fit_transform(Y_val)\n",
    "Y_test = label_encoder.fit_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model, X_train, Y_train, X_val, Y_val, X_test, Y_test):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, Y_train)\n",
    "    time_taken = time.time() - start_time\n",
    "    Y_val_pred = model.predict(X_val)\n",
    "    Y_test_pred = model.predict(X_test)\n",
    "    val_acc = accuracy_score(Y_val, Y_val_pred)\n",
    "    test_acc = accuracy_score(Y_test, Y_test_pred)\n",
    "    recall_val = recall_score(Y_val, Y_val_pred, average='macro')\n",
    "    recall_test = recall_score(Y_test, Y_test_pred, average='macro')\n",
    "    precision_val = precision_score(Y_val, Y_val_pred, average='macro')\n",
    "    precision_test = precision_score(Y_test, Y_test_pred, average='macro')\n",
    "    f1_val = f1_score(Y_val, Y_val_pred, average='macro')\n",
    "    f1_test = f1_score(Y_test, Y_test_pred, average='macro')\n",
    "    conf_matrix_val = confusion_matrix(Y_val, Y_val_pred)\n",
    "    conf_matrix_test = confusion_matrix(Y_test, Y_test_pred)\n",
    "    print('We have used the following model: {}'.format(model))\n",
    "    print('Test Accuracy: {}'.format(test_acc))\n",
    "    print('F1 Score: {}'.format(f1_test))\n",
    "    print('Confusion Matrix: {}'.format(conf_matrix_test))\n",
    "    print('Time taken (seconds): {}'.format(round(time_taken, 2)))\n",
    "    return val_acc, test_acc, recall_val, recall_test, precision_val, precision_test, f1_val, f1_test, conf_matrix_val, conf_matrix_test, time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=100, max_features=254, criterion='gini', min_samples_split=5),\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=200, max_features=254, criterion='gini', min_samples_split=5),\n",
    "    KNeighborsClassifier(n_neighbors=10, weights='distance', algorithm='auto'),\n",
    "    KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto'),\n",
    "    # SVC(C=1.0, kernel='rbf', gamma='scale'),\n",
    "    # SVC(C=10, kernel='rbf', gamma='scale'),\n",
    "    ExtraTreesClassifier(max_depth=10, n_estimators=100, max_features=16, criterion='entropy', min_samples_split=5),\n",
    "    XGBClassifier(max_depth=10, n_estimators=500, learning_rate=0.01, subsample=0.8, colsample_bytree=0.8, gamma = 0.1),\n",
    "    GradientBoostingClassifier(max_depth=10, n_estimators=100, learning_rate=0.01, subsample=0.5, min_impurity_decrease = 0.1),\n",
    "    AdaBoostClassifier(n_estimators=500, learning_rate=0.1),\n",
    "    MLPClassifier(hidden_layer_sizes=(100, 100, 100), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, max_iter=500),\n",
    "    MLPClassifier(hidden_layer_sizes=(100, 100, 100), activation='sigmoid', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, max_iter=1000),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Random Forest', 'Random Forest', 'KNN', 'KNN', 'SVM', 'SVM', 'Extra Trees', 'XGBoost', 'Gradient Boosting', 'AdaBoost']\n",
    "result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have used the following model: RandomForestClassifier(max_depth=10, max_features=254, min_samples_split=5)\n",
      "Test Accuracy: 0.83725\n",
      "F1 Score: 0.8372328992814808\n",
      "Confusion Matrix: [[1654  346]\n",
      " [ 305 1695]]\n",
      "Time taken (seconds): 217.61\n",
      "We have used the following model: RandomForestClassifier(max_depth=10, max_features=254, min_samples_split=5,\n",
      "                       n_estimators=200)\n",
      "Test Accuracy: 0.83775\n",
      "F1 Score: 0.8377345746430019\n",
      "Confusion Matrix: [[1656  344]\n",
      " [ 305 1695]]\n",
      "Time taken (seconds): 433.15\n",
      "We have used the following model: KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
      "Test Accuracy: 0.857\n",
      "F1 Score: 0.8567473022411534\n",
      "Confusion Matrix: [[1798  202]\n",
      " [ 370 1630]]\n",
      "Time taken (seconds): 0.16\n",
      "We have used the following model: KNeighborsClassifier(n_neighbors=10)\n",
      "Test Accuracy: 0.847\n",
      "F1 Score: 0.8463805680455014\n",
      "Confusion Matrix: [[1821  179]\n",
      " [ 433 1567]]\n",
      "Time taken (seconds): 0.21\n",
      "We have used the following model: ExtraTreesClassifier(criterion='entropy', max_depth=10, max_features=16,\n",
      "                     min_samples_split=5)\n",
      "Test Accuracy: 0.71825\n",
      "F1 Score: 0.7134255003881278\n",
      "Confusion Matrix: [[1696  304]\n",
      " [ 823 1177]]\n",
      "Time taken (seconds): 2.45\n",
      "We have used the following model: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=0.1, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...)\n",
      "Test Accuracy: 0.86425\n",
      "F1 Score: 0.8642495842643518\n",
      "Confusion Matrix: [[1725  275]\n",
      " [ 268 1732]]\n",
      "Time taken (seconds): 7176.39\n"
     ]
    }
   ],
   "source": [
    "for model, model_name in zip(models, model_names):\n",
    "    val_acc, test_acc, recall_val, recall_test, precision_val, precision_test, f1_val, f1_test, conf_matrix_val, conf_matrix_test, time_taken = model_training(model, X_train, Y_train, X_val, Y_val, X_test, Y_test)\n",
    "    result.append({'Model': model_name, 'Validation Accuracy': val_acc, 'Test Accuracy': test_acc, 'Validation Recall': recall_val, 'Test Recall': recall_test, 'Validation Precision': precision_val, 'Test Precision': precision_test, 'Validation F1': f1_val, 'Test F1': f1_test, 'Validation Confusion Matrix': conf_matrix_val, 'Test Confusion Matrix': conf_matrix_test, 'Time Taken': time_taken})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(resul)\n",
    "result_df.to_csv('result_1D_STFT.csv', index=False)\n",
    "result_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
