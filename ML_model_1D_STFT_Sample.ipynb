{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'StackNetClassifier' from 'pystacknet.pystacknet' (d:\\Study\\Code\\Python\\Sound_Classification_Bee_Qeen_Queenless\\pystacknet\\pystacknet\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mxgboost\u001b[39;00m \u001b[39mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m     22\u001b[0m \u001b[39m# import StackNetClassifier\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpystacknet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpystacknet\u001b[39;00m \u001b[39mimport\u001b[39;00m StackNetClassifier\n\u001b[0;32m     25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m     26\u001b[0m warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'StackNetClassifier' from 'pystacknet.pystacknet' (d:\\Study\\Code\\Python\\Sound_Classification_Bee_Qeen_Queenless\\pystacknet\\pystacknet\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# import StackNetClassifier\n",
    "from pystacknet.pystacknet import StackNetClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Study\\\\Code\\\\Python\\\\Sound_Classification_Bee_Qeen_Queenless/dataset/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd() + \"/dataset/\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Study\\Code\\Python\\Sound_Classification_Bee_Qeen_Queenless/dataset//train\n",
      "d:\\Study\\Code\\Python\\Sound_Classification_Bee_Qeen_Queenless/dataset//val\n",
      "d:\\Study\\Code\\Python\\Sound_Classification_Bee_Qeen_Queenless/dataset//test\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = path+\"/train\"\n",
    "VALIDATION_DIR = path + \"/val\"\n",
    "TEST_DIR = path + \"/test\"\n",
    "\n",
    "print(TRAINING_DIR)\n",
    "print(VALIDATION_DIR)\n",
    "print(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(path):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for folder in os.listdir(path):\n",
    "        for file in os.listdir(os.path.join(path, folder)):\n",
    "            feature = np.load(os.path.join(path, folder, file))\n",
    "            label = folder\n",
    "            X.append(feature)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = data_loader(TRAINING_DIR)\n",
    "X_val, Y_val = data_loader(VALIDATION_DIR)\n",
    "X_test, Y_test = data_loader(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14000 training samples and 14000 training labels\n",
      "There are 2000 validation samples and 2000 validation labels\n",
      "There are 4000 testing samples and 4000 testing labels\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} training samples and {} training labels\".format(len(X_train), len(Y_train)))\n",
    "print(\"There are {} validation samples and {} validation labels\".format(len(X_val), len(Y_val)))\n",
    "print(\"There are {} testing samples and {} testing labels\".format(len(X_test), len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (14000, 64575)\n",
      "Shape of X_val: (2000, 64575)\n",
      "Shape of X_test: (4000, 64575)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_val = np.array(Y_val)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "print(\"Shape of X_train: {}\".format(X_train.shape))\n",
    "print(\"Shape of X_val: {}\".format(X_val.shape))\n",
    "print(\"Shape of X_test: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset\n",
    "shuffle_index = np.random.permutation(len(X_train))\n",
    "X_train, Y_train = X_train[shuffle_index], Y_train[shuffle_index]\n",
    "shuffle_index = np.random.permutation(len(X_val))\n",
    "X_val, Y_val = X_val[shuffle_index], Y_val[shuffle_index]\n",
    "shuffle_index = np.random.permutation(len(X_test))\n",
    "X_test, Y_test = X_test[shuffle_index], Y_test[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "Y_train = label_encoder.fit_transform(Y_train)\n",
    "Y_val = label_encoder.fit_transform(Y_val)\n",
    "Y_test = label_encoder.fit_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model, X_train, Y_train, X_val, Y_val, X_test, Y_test):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, Y_train)\n",
    "    time_taken = time.time() - start_time\n",
    "    Y_val_pred = model.predict(X_val)\n",
    "    Y_test_pred = model.predict(X_test)\n",
    "    val_acc = accuracy_score(Y_val, Y_val_pred)\n",
    "    test_acc = accuracy_score(Y_test, Y_test_pred)\n",
    "    recall_val = recall_score(Y_val, Y_val_pred, average='macro')\n",
    "    recall_test = recall_score(Y_test, Y_test_pred, average='macro')\n",
    "    precision_val = precision_score(Y_val, Y_val_pred, average='macro')\n",
    "    precision_test = precision_score(Y_test, Y_test_pred, average='macro')\n",
    "    f1_val = f1_score(Y_val, Y_val_pred, average='macro')\n",
    "    f1_test = f1_score(Y_test, Y_test_pred, average='macro')\n",
    "    conf_matrix_val = confusion_matrix(Y_val, Y_val_pred)\n",
    "    conf_matrix_test = confusion_matrix(Y_test, Y_test_pred)\n",
    "    print('We have used the following model: {}'.format(model))\n",
    "    print('Test Accuracy: {}'.format(test_acc))\n",
    "    print('F1 Score: {}'.format(f1_test))\n",
    "    print('Confusion Matrix: {}'.format(conf_matrix_test))\n",
    "    print('Time taken (seconds): {}'.format(round(time_taken, 2)))\n",
    "    return val_acc, test_acc, recall_val, recall_test, precision_val, precision_test, f1_val, f1_test, conf_matrix_val, conf_matrix_test, time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=100, max_features=16, criterion='entropy', mean_samples_split=5),\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=50, max_features=254, criterion='gini, mean_samples_split=2'),\n",
    "    KNeighborsClassifier(n_neighbors=10, weights='distance', algorithm='auto'),\n",
    "    KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto'),\n",
    "    SVC(C=1.0, kernel='rbf', gamma='scale'),\n",
    "    SVC(C=10, kernel='rbf', gamma='scale'),\n",
    "    ExtraTreesClassifier(max_depth=10, n_estimators=100, max_features=16, criterion='entropy', mean_samples_split=5),\n",
    "    XGBClassifier(max_depth=10, n_estimators=500, learning_rate=0.01, subsample=0.8, colsample_bytree=0.8, gamma = 0.1),\n",
    "    GradientBoostingClassifier(max_depth=10, n_estimators=100, learning_rate=0.01, subsample=0.5, min_impurity_decrease = 0.1),\n",
    "    AdaBoostClassifier(n_estimators=500, learning_rate=0.1, bootstrap=True, criterion='entropy'),\n",
    "    MLPClassifier(hidden_layer_sizes=(100, 100, 100), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, max_iter=500),\n",
    "    MLPClassifier(hidden_layer_sizes=(100, 100, 100), activation='sigmoid', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, max_iter=1000),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Random Forest', 'Random Forest', 'KNN', 'KNN', 'SVM', 'SVM', 'Extra Trees', 'XGBoost', 'Gradient Boosting', 'AdaBoost']\n",
    "result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, model_name in zip(models, model_names):\n",
    "    val_acc, test_acc, recall_val, recall_test, precision_val, precision_test, f1_val, f1_test, conf_matrix_val, conf_matrix_test, time_taken = model_training(model, X_train, Y_train, X_val, Y_val, X_test, Y_test)\n",
    "    resul.append({'Model': model_name, 'Validation Accuracy': val_acc, 'Test Accuracy': test_acc, 'Validation Recall': recall_val, 'Test Recall': recall_test, 'Validation Precision': precision_val, 'Test Precision': precision_test, 'Validation F1': f1_val, 'Test F1': f1_test, 'Validation Confusion Matrix': conf_matrix_val, 'Test Confusion Matrix': conf_matrix_test, 'Time Taken': time_taken})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(resul)\n",
    "result_df.to_csv('result_1D_STFT.csv', index=False)\n",
    "result_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # StackNetClassifier(classifiers=[RandomForestClassifier(max_depth=10, n_estimators=100, max_features=16, criterion='entropy', mean_samples_split=5),\n",
    "    #                                 RandomForestClassifier(max_depth=10, n_estimators=50, max_features=254, criterion='gini, mean_samples_split=2'),\n",
    "    #                                 KNeighborsClassifier(n_neighbors=10, weights='distance', algorithm='auto'),\n",
    "    #                                 KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = StackNetClassifier(classifiers=[RandomForestClassifier(max_depth=10, n_estimators=100, max_features=16, criterion='entropy', mean_samples_split=5),\n",
    "#                                     RandomForestClassifier(max_depth=10, n_estimators=50, max_features=254, criterion='gini, mean_samples_split=2'),\n",
    "#                                     KNeighborsClassifier(n_neighbors=10, weights='distance', algorithm='auto'),\n",
    "#                                     KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto'),\n",
    "#                                     MLPClassifier(hidden_layer_sizes=(100, 100, 100), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, max_iter=500),\n",
    "#                                     MLPClassifier(hidden_layer_sizes=(100, 100, 100), activation='sigmoid', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, max_iter=1000)\n",
    "#                                        ], metric = 'accuracy', folds = 4, restacking = False, use_retraining = False, use_proba = True, random_state = 2021, n_jobs = -1, verbose = 1)                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(y_true_train, y_pred_train, y_true_val, y_pred_val, model_name):\n",
    "    \"\"\"\n",
    "    Plots the ROC Curve given predictions and labels\n",
    "    \"\"\"\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_true_train, y_pred_train, pos_label=1)\n",
    "    fpr_val, tpr_val, _ = roc_curve(y_true_val, y_pred_val, pos_label=1)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr_train, tpr_train, color='black',\n",
    "             lw=2, label=f\"ROC train curve (AUC = {round(roc_auc_score(y_true_train, y_pred_train), 4)})\")\n",
    "    plt.plot(fpr_val, tpr_val, color='darkorange',\n",
    "             lw=2, label=f\"ROC validation curve (AUC = {round(roc_auc_score(y_true_val, y_pred_val), 4)})\")\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.title(f'ROC Plot for {model_name}', weight=\"bold\", fontsize=20)\n",
    "    plt.legend(loc=\"lower right\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curve(Y_train, Y_train_pred, Y_val, Y_val_pred, 'Random Forest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
